{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:42:51.494276Z",
     "start_time": "2024-08-01T18:42:20.623296Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from IPython.display import Image, display\n",
    "# import fitz  # PyMuPDF\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "api_key = 'your_api-key'\n",
    "os.environ[\"OPENAI_API_KEY\"]  = api_key"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:42:51.505755Z",
     "start_time": "2024-08-01T18:42:51.494789Z"
    }
   },
   "id": "712723600688536c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_LIMITS = {\n",
    "    \"gpt-3.5-turbo-0125\": 16_385,\n",
    "    \"gpt-4-turbo-2024-04-09\": 128_000,\n",
    "    \"gpt-4o-2024-05-13\": 128_000,\n",
    "    \"gpt-4o-mini-2024-07-18\": 128_000\n",
    "}\n",
    "\n",
    "# The cost per token for each model input.\n",
    "MODEL_COST_PER_INPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000005,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00001,\n",
    "    \"gpt-4o-2024-05-13\": 0.000005,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.00000015\n",
    "}\n",
    "\n",
    "# The cost per token for each model output.\n",
    "MODEL_COST_PER_OUTPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000015,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00003,\n",
    "    \"gpt-4o-2024-05-13\": 0.000015,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.0000006\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T20:57:02.127103Z",
     "start_time": "2024-08-01T20:57:01.948611Z"
    }
   },
   "id": "a6132e2e406890f6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "74"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"./data_filted_csv/kaggle_data_eval.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(eval(line))\n",
    "        \n",
    "len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:42:51.529153Z",
     "start_time": "2024-08-01T18:42:51.511326Z"
    }
   },
   "id": "fdb261df9c451d6a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33muser_proxy\u001B[0m (to assistant):\n",
      "\n",
      "What date is today? Compare the year-to-date gain for META and TESLA.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001B[33massistant\u001B[0m (to user_proxy):\n",
      "\n",
      "To get today's date, you can use Python to print the current date. Then, we can proceed to compare the year-to-date gain for META and TESLA. Let's start by getting today's date.\n",
      "\n",
      "```python\n",
      "# Get today's date\n",
      "import datetime\n",
      "today = datetime.date.today()\n",
      "print(today)\n",
      "```\n",
      "\n",
      "After getting today's date, we need to find the year-to-date gain for META and TESLA. To do this, we would typically need access to the stock prices at the beginning of the year and the current stock prices for META and TESLA. Since this information is not provided, we cannot directly calculate the year-to-date gain for these stocks. You can check financial news websites or stock market platforms to find the necessary stock price information for META and TESLA to calculate the year-to-date gain.\n",
      "\n",
      "If you can provide the stock prices at the beginning of the year and the current stock prices for META and TESLA, I can help you calculate the year-to-date gain for both companies.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"tags\": [\"gpt-4\"]},  # comment out to get all\n",
    ")\n",
    "\n",
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        # the executor to run the generated code\n",
    "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    },\n",
    ")\n",
    "start = time.time()\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")\n",
    "consume = time.time() - start"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e9b8045a2c9400",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatResult(chat_id=None, chat_history=[{'content': 'What date is today? Compare the year-to-date gain for META and TESLA.', 'role': 'assistant'}, {'content': \"To get today's date, you can use Python to print the current date. Then, we can proceed to compare the year-to-date gain for META and TESLA. Let's start by getting today's date.\\n\\n```python\\n# Get today's date\\nimport datetime\\ntoday = datetime.date.today()\\nprint(today)\\n```\\n\\nAfter getting today's date, we need to find the year-to-date gain for META and TESLA. To do this, we would typically need access to the stock prices at the beginning of the year and the current stock prices for META and TESLA. Since this information is not provided, we cannot directly calculate the year-to-date gain for these stocks. You can check financial news websites or stock market platforms to find the necessary stock price information for META and TESLA to calculate the year-to-date gain.\\n\\nIf you can provide the stock prices at the beginning of the year and the current stock prices for META and TESLA, I can help you calculate the year-to-date gain for both companies.\\n\\nTERMINATE\", 'role': 'user'}], summary=\"Today's date can be obtained using Python. To compare the year-to-date gain for META and TESLA, you would need the stock prices at the beginning of the year and the current stock prices for both companies.\", cost={'usage_including_cached_inference': {'total_cost': 0.0007615, 'gpt-3.5-turbo-0125': {'cost': 0.0007615, 'prompt_tokens': 752, 'completion_tokens': 257, 'total_tokens': 1009}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_res"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e92fb802965df5",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "prompt_tokens = chat_res.cost['usage_including_cached_inference'][model]['prompt_tokens']\n",
    "completion_tokens = chat_res.cost['usage_including_cached_inference'][model]['completion_tokens']\n",
    "cost = chat_res.cost['usage_including_cached_inference'][model]['cost']\n",
    "summary = chat_res.summary\n",
    "history = chat_res.chat_history                "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T04:51:18.498932Z",
     "start_time": "2024-07-16T04:51:18.471010Z"
    }
   },
   "id": "9e31c75f84e55e65",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'total_cost': 0.0007615,\n 'gpt-3.5-turbo-0125': {'cost': 0.0007615,\n  'prompt_tokens': 752,\n  'completion_tokens': 257,\n  'total_tokens': 1009}}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_res.cost['usage_including_cached_inference']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T04:34:09.351010Z",
     "start_time": "2024-07-16T04:34:09.292337Z"
    }
   },
   "id": "c254010f687d73b0",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.039936065673828125"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consume"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T00:13:38.185420Z",
     "start_time": "2024-07-16T00:13:38.178050Z"
    }
   },
   "id": "c2b1a688821311f5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_response(text, config_list):\n",
    "    assistant = autogen.AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        llm_config={\n",
    "            \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "            \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "            \"temperature\": 0,  # temperature for sampling\n",
    "        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    "    )\n",
    "    \n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        code_execution_config={\n",
    "            # the executor to run the generated code\n",
    "            \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "        },\n",
    "    )\n",
    "    # the assistant receives a message from the user_proxy, which contains the task description\n",
    "    chat_res = user_proxy.initiate_chat(\n",
    "            assistant,\n",
    "            message=text,\n",
    "            summary_method=\"reflection_with_llm\",\n",
    "        )\n",
    "    return chat_res\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:42:51.550245Z",
     "start_time": "2024-08-01T18:42:51.531874Z"
    }
   },
   "id": "478879b48a8e137b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path = \"please input your datapath\"\n",
    "\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "config_list = [{\"model\": model, \"api_key\": api_key}]\n",
    "total_cost = 0\n",
    "\n",
    "instruction = \"You are a data scientist. I have a data modeling task. You must give me the predicted results as a CSV file as detailed in the following content. You should try your best to predict the answer. I provide you with three files. One is training data, one is test data. There is also a sample file for submission\"\n",
    "\n",
    "save_path = \"./output_model/\"\n",
    "\n",
    "for id in tqdm(range(0, len(data))):\n",
    "# for id in tqdm([0]):\n",
    "    # print(sample)\n",
    "    name = data[id]['name']\n",
    "    with open(f\"./data_filted_csv/cleaned_task/{name}.txt\", \"r\") as f:\n",
    "        description = f.read()\n",
    "    \n",
    "    text = (f\"\\n \\n All three data files can be found in the folder: {path}/data_filted_csv/data_resplit/{name}/. After the data modeling, please give me the prediction resutls for the test file. You must\"\n",
    "            f\" save the answer as a csv file. I won't run your code and you must run the code for the predicted results and give the submission file. The file should be saved in the path /Users/tencentintern/PycharmProjects/DSBench/kaggle_data/output_model/{model}-autoagent/{name}.csv\") \n",
    "    \n",
    "    all_context = instruction + \"\\n\" + description + \"\\n\" + text\n",
    "    input_t = all_context\n",
    "\n",
    "    # input_t = truncate_text(all_context, 2000)\n",
    "    start = time.time()\n",
    "    cost = 0\n",
    "    error = \"\"\n",
    "    prompt_tokens = completion_tokens = 0\n",
    "    try:\n",
    "        response = get_response(input_t, config_list)\n",
    "        prompt_tokens = response.cost['usage_including_cached_inference'][model]['prompt_tokens']\n",
    "        completion_tokens = response.cost['usage_including_cached_inference'][model]['completion_tokens']\n",
    "        cost = response.cost['usage_including_cached_inference'][model]['cost']\n",
    "        summary = response.summary\n",
    "        history = response.chat_history\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # time.sleep(3)\n",
    "        error = str(e)\n",
    "        # cost = 0\n",
    "        history = \"I cannot solve this task.\"\n",
    "        summary = \"I cannot solve this task.\"\n",
    "        print(history)\n",
    "        print(e)\n",
    "        time.sleep(3)\n",
    "                # all_mess.append(\"I cannot solve this task.\")\n",
    "    total_cost += cost\n",
    "    print(\"Total cost: \", total_cost)\n",
    "\n",
    "    if not os.path.exists(f\"{save_path}{model}-autoagent/\"):\n",
    "        os.makedirs(f\"{save_path}{model}-autoagent/\")\n",
    "    with open(f\"{save_path}{model}-autoagent/{name}.json\", \"w\") as f:\n",
    "        json.dump({\"name\": name, \"model\": model, \"input\": prompt_tokens,\n",
    "                            \"output\": completion_tokens, \"cost\": cost, \"time\": time.time()-start, \"error\": error, 'summary': summary, \"history\": history}, f)\n",
    "    \n",
    "\n",
    "    if total_cost > 100:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eee15c339d6795b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-2024-05-13\"\n",
    "# model = \"gpt-4-turbo-2024-04-09\"\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "config_list = [{\"model\": model, \"api_key\": api_key}]\n",
    "total_cost = 0\n",
    "\n",
    "instruction = \"You are a data scientist. I have a data modeling task. You must give me the predicted results as a CSV file as detailed in the following content. You should try your best to predict the answer. I provide you with three files. One is training data, one is test data. There is also a sample file for submission\"\n",
    "\n",
    "save_path = \"./output_model/\"\n",
    "\n",
    "\n",
    "for id in tqdm(range(0, len(data))):\n",
    "# for id in tqdm([0]):\n",
    "    # print(sample)\n",
    "    name = data[id]['name']\n",
    "    with open(f\"/Users/tencentintern/PycharmProjects/DSBench/kaggle_data/data_filted_csv/cleaned_task/{name}.txt\", \"r\") as f:\n",
    "        description = f.read()\n",
    "    \n",
    "    text = (f\"\\n \\n All three data files can be found in the folder: /Users/tencentintern/PycharmProjects/DSBench/kaggle_data/data_filted_csv/data_resplit/{name}/. After the data modeling, please give me the prediction resutls for the test file. You must\"\n",
    "            f\" save the answer as a csv file. I won't run your code and you must run the code for the predicted results and give the submission file. The file should be saved in the path /Users/tencentintern/PycharmProjects/DSBench/kaggle_data/output_model/{model}-autoagent/{name}.csv\") \n",
    "    \n",
    "    all_context = instruction + \"\\n\" + description + \"\\n\" + text\n",
    "    input_t = all_context\n",
    "\n",
    "    # input_t = truncate_text(all_context, 2000)\n",
    "    start = time.time()\n",
    "    cost = 0\n",
    "    error = \"\"\n",
    "    prompt_tokens = completion_tokens = 0\n",
    "    try:\n",
    "        response = get_response(input_t, config_list)\n",
    "        prompt_tokens = response.cost['usage_including_cached_inference'][model]['prompt_tokens']\n",
    "        completion_tokens = response.cost['usage_including_cached_inference'][model]['completion_tokens']\n",
    "        # cost = response.cost['usage_including_cached_inference'][model]['cost']\n",
    "        cost = prompt_tokens + completion_tokens\n",
    "        summary = response.summary\n",
    "        history = response.chat_history\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # time.sleep(3)\n",
    "        error = str(e)\n",
    "        # cost = 0\n",
    "        history = \"I cannot solve this task.\"\n",
    "        summary = \"I cannot solve this task.\"\n",
    "        print(history)\n",
    "        print(e)\n",
    "        time.sleep(3)\n",
    "                # all_mess.append(\"I cannot solve this task.\")\n",
    "    total_cost += cost\n",
    "    print(\"Total cost: \", total_cost)\n",
    "\n",
    "    if not os.path.exists(f\"{save_path}{model}-autoagent/\"):\n",
    "        os.makedirs(f\"{save_path}{model}-autoagent/\")\n",
    "    with open(f\"{save_path}{model}-autoagent/{name}.json\", \"w\") as f:\n",
    "        json.dump({\"name\": name, \"model\": model, \"input\": prompt_tokens,\n",
    "                            \"output\": completion_tokens, \"cost\": cost, \"time\": time.time()-start, \"error\": error, 'summary': summary, \"history\": history}, f)\n",
    "    \n",
    "\n",
    "    if total_cost > 100:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffa0308834c54d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ffdbe44026bd8de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
