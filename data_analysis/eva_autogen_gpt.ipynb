{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T02:38:25.552143Z",
     "start_time": "2024-08-02T02:38:21.191988Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please replace all path to your path before stating to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from IPython.display import Image, display\n",
    "# import fitz  # PyMuPDF\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'your-api'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd214afd1402a68"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gpt_tokenize(string: str, encoding) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def find_jpg_files(directory):\n",
    "    jpg_files = [file for file in os.listdir(directory) if file.lower().endswith('.jpg') or file.lower().endswith('.png')]\n",
    "    return jpg_files if jpg_files else None\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def find_excel_files(directory):\n",
    "    jpg_files = [file for file in os.listdir(directory) if (file.lower().endswith('xlsx') or file.lower().endswith('xlsb') or file.lower().endswith('xlsm')) and not \"answer\" in file.lower()]\n",
    "    return jpg_files if jpg_files else None\n",
    "\n",
    "def read_excel(file_path):\n",
    "    # 读取Excel文件中的所有sheet\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheets = {}\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        sheets[sheet_name] = xls.parse(sheet_name)\n",
    "    return sheets\n",
    "\n",
    "def dataframe_to_text(df):\n",
    "    # 将DataFrame转换为文本\n",
    "    text = df.to_string(index=False)\n",
    "    return text\n",
    "\n",
    "def combine_sheets_text(sheets):\n",
    "    # 将所有sheet的文本内容组合起来\n",
    "    combined_text = \"\"\n",
    "    for sheet_name, df in sheets.items():\n",
    "        sheet_text = dataframe_to_text(df)\n",
    "        combined_text += f\"Sheet name: {sheet_name}\\n{sheet_text}\\n\\n\"\n",
    "    return combined_text\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def truncate_text(text, max_tokens=128000):\n",
    "    # 计算当前文本的token数\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_tokens:\n",
    "        # 截断文本以确保不超过最大token数\n",
    "        text = ' '.join(tokens[-max_tokens:])\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T02:38:25.603037Z",
     "start_time": "2024-08-02T02:38:25.548449Z"
    }
   },
   "id": "712723600688536c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_LIMITS = {\n",
    "    \"gpt-3.5-turbo-0125\": 16_385,\n",
    "    \"gpt-4-turbo-2024-04-09\": 128_000,\n",
    "    \"gpt-4o-2024-05-13\": 128_000,\n",
    "    \"gpt-4o-mini-2024-07-18\": 128_000\n",
    "}\n",
    "\n",
    "# The cost per token for each model input.\n",
    "MODEL_COST_PER_INPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000005,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00001,\n",
    "    \"gpt-4o-2024-05-13\": 0.000005,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.00000015\n",
    "}\n",
    "\n",
    "# The cost per token for each model output.\n",
    "MODEL_COST_PER_OUTPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000015,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00003,\n",
    "    \"gpt-4o-2024-05-13\": 0.000015,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.0000006\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T02:38:25.619289Z",
     "start_time": "2024-08-02T02:38:25.561530Z"
    }
   },
   "id": "a6132e2e406890f6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "43"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "with open(\"./data.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        samples.append(eval(line.strip()))\n",
    "len(samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T02:38:25.639815Z",
     "start_time": "2024-08-02T02:38:25.575965Z"
    }
   },
   "id": "fdb261df9c451d6a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# config_list = autogen.config_list_from_json(\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#     \"OAI_CONFIG_LIST\",\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#     filter_dict={\"tags\": [\"gpt-4\"]},  # comment out to get all\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# create an AssistantAgent named \"assistant\"\u001B[39;00m\n\u001B[1;32m      7\u001B[0m assistant \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mAssistantAgent(\n\u001B[1;32m      8\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m     llm_config\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcache_seed\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m41\u001B[39m,  \u001B[38;5;66;03m# seed for caching and reproducibility\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_list\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mconfig_list\u001B[49m,  \u001B[38;5;66;03m# a list of OpenAI API configurations\u001B[39;00m\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m,  \u001B[38;5;66;03m# temperature for sampling\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     },  \u001B[38;5;66;03m# configuration for autogen's enhanced inference API which is compatible with OpenAI API\u001B[39;00m\n\u001B[1;32m     14\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# create a UserProxyAgent instance named \"user_proxy\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m user_proxy \u001B[38;5;241m=\u001B[39m autogen\u001B[38;5;241m.\u001B[39mUserProxyAgent(\n\u001B[1;32m     18\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser_proxy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     19\u001B[0m     human_input_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNEVER\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     25\u001B[0m     },\n\u001B[1;32m     26\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'config_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"tags\": [\"gpt-4\"]},  # comment out to get all\n",
    ")\n",
    "\n",
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        # the executor to run the generated code\n",
    "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    },\n",
    ")\n",
    "start = time.time()\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")\n",
    "consume = time.time() - start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T03:52:07.971602Z",
     "start_time": "2024-07-31T03:52:07.404538Z"
    }
   },
   "id": "c2e9b8045a2c9400",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chat_res"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e92fb802965df5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "prompt_tokens = chat_res.cost['usage_including_cached_inference'][model]['prompt_tokens']\n",
    "completion_tokens = chat_res.cost['usage_including_cached_inference'][model]['completion_tokens']\n",
    "cost = chat_res.cost['usage_including_cached_inference'][model]['cost']\n",
    "summary = chat_res.summary\n",
    "history = chat_res.chat_history                "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T04:51:18.498932Z",
     "start_time": "2024-07-16T04:51:18.471010Z"
    }
   },
   "id": "9e31c75f84e55e65",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chat_res.cost['usage_including_cached_inference']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c254010f687d73b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.039936065673828125"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consume"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T00:13:38.185420Z",
     "start_time": "2024-07-16T00:13:38.178050Z"
    }
   },
   "id": "c2b1a688821311f5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_response(text, config_list):\n",
    "    assistant = autogen.AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        llm_config={\n",
    "            \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "            \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "            \"temperature\": 0,  # temperature for sampling\n",
    "        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    "    )\n",
    "    \n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        code_execution_config={\n",
    "            # the executor to run the generated code\n",
    "            \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "        },\n",
    "    )\n",
    "    # the assistant receives a message from the user_proxy, which contains the task description\n",
    "    chat_res = user_proxy.initiate_chat(\n",
    "            assistant,\n",
    "            message=text,\n",
    "            summary_method=\"reflection_with_llm\",\n",
    "        )\n",
    "    return chat_res\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T02:38:31.466683Z",
     "start_time": "2024-08-02T02:38:31.338517Z"
    }
   },
   "id": "478879b48a8e137b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for id in tqdm(range(31, len(samples))):\n",
    "# for id in tqdm(range(34, 38)):\n",
    "# model = \"gpt-3.5-turbo-0125-autoagent\"\n",
    "# model = \"gpt-4o-2024-05-13\"\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "# model = \"gpt-4o-mini-2024-07-18\"\n",
    "config_list = [{\"model\": model, \"api_key\": \"your-api-key\"}]\n",
    "total_cost = 0\n",
    "for id in tqdm(range(36, len(samples))):\n",
    "# for id in tqdm([0]):\n",
    "    # print(sample)\n",
    "\n",
    "    sample =samples[id]\n",
    "    if len(sample[\"questions\"]) > 0:\n",
    "\n",
    "        image = find_jpg_files(os.path.join(\"./data\", sample[\"id\"]))\n",
    "\n",
    "        \n",
    "        excels = find_excel_files(os.path.join(\"./data\", sample[\"id\"]))\n",
    "\n",
    "\n",
    "        introduction = read_txt(os.path.join(\"./data\", sample[\"id\"], \"introduction.txt\"))\n",
    "        questions = []\n",
    "        for question_name in sample[\"questions\"]:\n",
    "            questions.append(read_txt(os.path.join(\"./data\", sample[\"id\"], question_name+\".txt\")))\n",
    "        \n",
    "    \n",
    "        \n",
    "        text = f\"The introduction is detailed as follows. \\n {introduction}\" \n",
    "        if excels:\n",
    "            text += \"\\n \\n The worksheet can be obtained in the path: \"\n",
    "            for excel in excels:\n",
    "                text += f\" {os.path.join('./data',  sample['id'], excel)}\"\n",
    "    \n",
    "        if image:\n",
    "            text += f\"\\n The image can be obtained in the path: {os.path.join('./data',  sample['id'], image[0])} \\n\"\n",
    "        \n",
    "        question_content = \"\"    \n",
    "        # print(workbooks)\n",
    "        answers = []\n",
    "        all_mess = []\n",
    "        for question in tqdm(questions):\n",
    "            # question_content += question\n",
    "    \n",
    "            all_context = text + f\"The question is detailed as follows. \\n {question} \\nPlease answer the question. \"\n",
    "            input_t = all_context\n",
    "            # input_t = truncate_text(all_context, 2000)\n",
    "            start = time.time()\n",
    "            cost = 0\n",
    "            prompt_tokens = completion_tokens = 0\n",
    "            try:\n",
    "                response = get_response(input_t, config_list)\n",
    "\n",
    "                prompt_tokens = response.cost['usage_including_cached_inference'][model]['prompt_tokens']\n",
    "                completion_tokens = response.cost['usage_including_cached_inference'][model]['completion_tokens']\n",
    "                cost = response.cost['usage_including_cached_inference'][model]['cost']\n",
    "                summary = response.summary\n",
    "                history = response.chat_history\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # cost = 0\n",
    "                history = \"I cannot solve this task.\"\n",
    "                summary = \"I cannot solve this task.\"\n",
    "                # all_mess.append(\"I cannot solve this task.\")\n",
    "            total_cost += cost\n",
    "            print(\"Total cost: \", total_cost)\n",
    "            answers.append({\"id\": sample[\"id\"], \"model\": model, \"input\": prompt_tokens,\n",
    "                            \"output\": completion_tokens, \"cost\": cost, \"time\": time.time()-start, 'summary': summary, \"history\": history })\n",
    "            # break\n",
    "    save_path = os.path.join(\"./save_process\", model+\"-autoagent\")\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    with open(os.path.join(save_path, sample['id'] + \".json\"), \"w\") as f:\n",
    "        for answer in answers:\n",
    "            json.dump(answer, f)\n",
    "            f.write(\"\\n\")\n",
    "    # if total_cost > 100:\n",
    "    #     break\n",
    "    # break\n",
    "            # the assistant receives a message from the user_proxy, which contains the task description\n",
    "    # print(answers)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eee15c339d6795b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9864e811bc0428"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
